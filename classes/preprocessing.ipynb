{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook prepared by Henrique Lopes Cardoso (hlc@fe.up.pt).\n",
    "\n",
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "*Tokenization* is the process of spliting an input text into tokens (words or other relevant elements, such as punctuation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making use of regular expressions\n",
    "\n",
    "We can tokenize a piece of text by using a regular expression tokenizer, such as the one available in **NLTK**.\n",
    "\n",
    "For starters, let's stick to alphanumerical sequences of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "['That', 'U', 'S', 'A', 'poster', 'print', 'costs', '12', '40']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import regexp_tokenize\n",
    "\n",
    "text = 'That U.S.A. poster-print costs $12.40...'\n",
    "\n",
    "pattern = '[a-zA-Z0-9_]+'\n",
    "tokens = regexp_tokenize(text, pattern)\n",
    "print(len(tokens))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can refine the regular expression to obtain a more sensible tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\n"
     ]
    }
   ],
   "source": [
    "pattern = r'''(?x)           # set flag to allow verbose regexps\n",
    "        (?:[A-Z]\\.)+         # abbreviations, e.g. U.S.A.\n",
    "        | \\w+(?:-\\w+)*       # words with optional internal hyphens\n",
    "        | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
    "        | \\.\\.\\.             # ellipsis\n",
    "        | [][.,;\"'?():-_`]   # these are separate tokens; includes ], [\n",
    "        '''\n",
    "\n",
    "tokens = regexp_tokenize(text, pattern)\n",
    "print(len(tokens))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using NLTK\n",
    "\n",
    "NLTK also includes a word tokenizer, which gets roughly the same result (it finds \"words\" and punctuation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$', '12.40', '...']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "text = 'That U.S.A. poster-print costs $12.40...'\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(len(tokens))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'do', \"n't\", 'think', 'we', \"'re\", 'flying', 'today', '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"I don't think we're flying today.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try [other tokenizers](https://www.nltk.org/api/nltk.tokenize.html) available in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That',\n",
       " 'U',\n",
       " '.',\n",
       " 'S',\n",
       " '.',\n",
       " 'A',\n",
       " '.',\n",
       " 'poster',\n",
       " '-',\n",
       " 'print',\n",
       " 'costs',\n",
       " '$',\n",
       " '12',\n",
       " '.',\n",
       " '40',\n",
       " '...']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try out the wordpunct tokenizer\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "wordpunct_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a sentence from the user and tokenize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You typed 4 words: ['ola', 'sou', 'a', 'neni']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "s = input(\"Enter some text:\")\n",
    "tokens = word_tokenize(s)\n",
    "\n",
    "print(\"You typed\", len(tokens), \"words:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence segmentation\n",
    "\n",
    "We may also be interested in spliting the text into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello.', 'Are you Mr. Smith?', 'Just to let you know that I have finished my M.Sc.', 'and Ph.D. on AI.', 'I loved it!']\n",
      "Number of sentences: 5\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "text = \"Hello. Are you Mr. Smith? Just to let you know that I have finished my M.Sc. and Ph.D. on AI. I loved it!\"\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(sentences)\n",
    "print(\"Number of sentences:\", len(sent_tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimenting with long texts\n",
    "\n",
    "We can try downloading a book from Project Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176812\n",
      "ï»¿The Project Gutenberg eBook of Crime and Punishment, by Fyodor Dostoevsky\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "\n",
    "print(len(raw))\n",
    "print(raw[:75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many sentences are there? Printout the second sentence (index 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this eBook or online at\n",
      "www.gutenberg.org.\n"
     ]
    }
   ],
   "source": [
    "# insert your code here\n",
    "sentences = sent_tokenize(raw)\n",
    "\n",
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many tokens are there? What is the index of the first token in the second sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens: 257058\n",
      "index of the first token in the second sentence: 42\n"
     ]
    }
   ],
   "source": [
    "# insert your code here\n",
    "tokens = word_tokenize(raw)\n",
    "\n",
    "print(\"number of tokens:\", len(tokens))\n",
    "\n",
    "s = sentences[1]\n",
    "sentence_tokens = word_tokenize(s)\n",
    "print(\"index of the first token in the second sentence:\", tokens.index(sentence_tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with multi-word expressions (MWE)\n",
    "\n",
    "Sometimes we want certain words to stick together when tokenizing, such as in multi-word names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"Good muffins cost $3.88\\nin New York.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to do it is to suply our own lexicon and make use of NLTK's [MWE tokenizer](https://www.nltk.org/api/nltk.tokenize.mwe.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New York', '.']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "s = \"Good muffins cost $3.88\\nin New York.\"\n",
    "mwe = MWETokenizer([('New', 'York'), ('Hong', 'Kong')], separator=' ')\n",
    "\n",
    "[mwe.tokenize(word_tokenize(sent)) for sent in sent_tokenize(s)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out your own multi-word expressions to tokenize text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Estamos',\n",
       "  'a',\n",
       "  'estudar',\n",
       "  'Engenharia Informatica',\n",
       "  'na',\n",
       "  'Universidade do Porto']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try out your own multi-word expressions\n",
    "s = \"Estamos a estudar Engenharia Informatica na Universidade do Porto\"\n",
    "mwe = MWETokenizer([('Engenharia', 'Informatica'), ('Universidade', 'do', 'Porto')], separator=' ')\n",
    "\n",
    "[mwe.tokenize(word_tokenize(sent)) for sent in sent_tokenize(s)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "*Stemming* and *Lemmatization* are techniques used to normalize tokens, so as to reduce the size of the vocabulary.\n",
    "Whereas lemmatization is a process of finding the root of the word, stemming typically applies a set of transformation rules that aim to cut off word final affixes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "\n",
    "NLTK includes one of the most well-known stemmers: the [Porter stemmer](https://www.emerald.com/insight/content/doi/10.1108/00330330610681286/full/pdf?casa_token=eT_IPtH_eLEAAAAA:Z3lAtxWdxf0FL479mL-A7tC-_QRzxNeeyC2DFLyWwGBlcj6DQcwu2Bnq37waDPcXKOnXkMMDtKGyCaYGZtYcb3lgBZ9uaHKUNO0JCMivSdPE4HTe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# initialize the Porter Stemmer\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use an illustrative piece of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original word list: ['The', 'European', 'Commission', 'has', 'funded', 'a', 'numerical', 'study', 'to', 'analyze', 'the', 'purchase', 'of', 'a', 'pipe', 'organ', 'with', 'no', 'noise', 'for', 'Europe', \"'s\", 'organization', '.', 'Numerous', 'donations', 'have', 'followed', 'the', 'analysis', 'after', 'a', 'noisy', 'debate', '.']\n",
      "\n",
      "Original number of distinct tokens: 31\n"
     ]
    }
   ],
   "source": [
    "sentence = '''The European Commission has funded a numerical study to analyze the purchase of a pipe organ with no noise\n",
    "for Europe's organization. Numerous donations have followed the analysis after a noisy debate.'''\n",
    "\n",
    "# tokenize: split the text into words\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "\n",
    "print(\"\\nOriginal word list:\", word_list)\n",
    "print(\"\\nOriginal number of distinct tokens:\", len(set(word_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we stem the tokens in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed text: the european commiss ha fund a numer studi to analyz the purchas of a pipe organ with no nois for europ 's organ . numer donat have follow the analysi after a noisi debat .\n",
      "\n",
      "Stemmed word list: ['the', 'european', 'commiss', 'ha', 'fund', 'a', 'numer', 'studi', 'to', 'analyz', 'the', 'purchas', 'of', 'a', 'pipe', 'organ', 'with', 'no', 'nois', 'for', 'europ', \"'s\", 'organ', '.', 'numer', 'donat', 'have', 'follow', 'the', 'analysi', 'after', 'a', 'noisi', 'debat', '.']\n",
      "\n",
      "Stemmed number of distinct tokens: 28\n"
     ]
    }
   ],
   "source": [
    "# stem list of words and join\n",
    "stemmed_output = ' '.join([porter.stem(w) for w in word_list])\n",
    "print(\"Stemmed text:\", stemmed_output)\n",
    "\n",
    "# tokenize: split the text into words\n",
    "stemmed_word_list = nltk.word_tokenize(stemmed_output)\n",
    "\n",
    "print(\"\\nStemmed word list:\", stemmed_word_list)\n",
    "print(\"\\nStemmed number of distinct tokens:\", len(set(stemmed_word_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the reduced vocabulary size. Some tokens are over-generalized (semantically different tokens that get the same stem), while others are under-generalized (semantically similar tokens that get different stems)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out [other stemmers](https://www.nltk.org/api/nltk.stem.html) available in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed text: the europ commit has fund a num study to analys the purchas of a pip org with no nois for europ 's org . num don hav follow the analys aft a noisy deb .\n",
      "\n",
      "Stemmed word list: ['the', 'europ', 'commit', 'has', 'fund', 'a', 'num', 'study', 'to', 'analys', 'the', 'purchas', 'of', 'a', 'pip', 'org', 'with', 'no', 'nois', 'for', 'europ', \"'s\", 'org', '.', 'num', 'don', 'hav', 'follow', 'the', 'analys', 'aft', 'a', 'noisy', 'deb', '.']\n",
      "\n",
      "Stemmed number of distinct tokens: 26\n"
     ]
    }
   ],
   "source": [
    "# try out other stemmers\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "st = LancasterStemmer()\n",
    "\n",
    "# stem list of words and join\n",
    "stemmed_output = ' '.join([st.stem(w) for w in word_list])\n",
    "print(\"Stemmed text:\", stemmed_output)\n",
    "\n",
    "# tokenize: split the text into words\n",
    "stemmed_word_list = nltk.word_tokenize(stemmed_output)\n",
    "\n",
    "print(\"\\nStemmed word list:\", stemmed_word_list)\n",
    "print(\"\\nStemmed number of distinct tokens:\", len(set(stemmed_word_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try a few for Portuguese:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\ineso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping stemmers\\rslp.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est mesm a gost dest unidad curricul , tod gost de unidad curricul interess .\n"
     ]
    }
   ],
   "source": [
    "# Portuguese stemmer: https://www.nltk.org/_modules/nltk/stem/rslp.html\n",
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "stemmer = RSLPStemmer()\n",
    "sentence = \"Estou mesmo a gostar desta unidade curricular, todos gostamos de unidades curriculares interessantes.\"\n",
    "\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "stemmed_output = ' '.join([stemmer.stem(w) for w in word_list])\n",
    "print(stemmed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estou mesm a gost dest unidad curricul , tod gost de unidad curricul interess .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"portuguese\")\n",
    "sentence = \"Estou mesmo a gostar desta unidade curricular, todos gostamos de unidades curriculares interessantes.\"\n",
    "\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "stemmed_output = ' '.join([stemmer.stem(w) for w in word_list])\n",
    "print(stemmed_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "NLTK includes a [lemmatizer based on WordNet](https://www.nltk.org/api/nltk.stem.wordnet.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ineso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Men', 'and', 'women', 'love', 'to', 'study', 'artificial', 'intelligence', 'while', 'studying', 'data', 'science', '.', 'My', 'feet', 'and', 'teeth', 'are', 'clean', '!']\n",
      "['Men', 'and', 'woman', 'love', 'to', 'study', 'artificial', 'intelligence', 'while', 'studying', 'data', 'science', '.', 'My', 'foot', 'and', 'teeth', 'are', 'clean', '!']\n"
     ]
    }
   ],
   "source": [
    "# WordNet lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"Men and women love to study artificial intelligence while studying data science. My feet and teeth are clean!\"\n",
    "\n",
    "# tokenize: Split the sentence into words\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "print(word_list)\n",
    "\n",
    "# lemmatize list of words\n",
    "lemmatized_output = [lemmatizer.lemmatize(w) for w in word_list]\n",
    "print(lemmatized_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the result with stemming applied to the same text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['men', 'and', 'women', 'lov', 'to', 'study', 'artificial', 'intelligenc', 'whil', 'studying', 'dat', 'scienc', '.', 'my', 'feet', 'and', 'teeth', 'are', 'clean', '!']\n"
     ]
    }
   ],
   "source": [
    "# compare with stemming\n",
    "stemmed_output = [stemmer.stem(w) for w in word_list]\n",
    "print(stemmed_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
